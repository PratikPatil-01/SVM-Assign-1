{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "935dcce5-5004-472a-aa2b-57dd82598344",
   "metadata": {},
   "source": [
    "### 2)\n",
    "The objective function of a linear Support Vector Machine (SVM) is to find the hyperplane that maximally separates the two classes in a binary classification problem. The goal is to determine the optimal hyperplane that maximizes the margin between the two classes while minimizing the classification error."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc2bb5d-e4fd-43d0-b873-7d6ebbcc0dfd",
   "metadata": {},
   "source": [
    "### 3)\n",
    "The kernel trick is a technique used in Support Vector Machines (SVMs) to implicitly map the input data into a higher-dimensional feature space without explicitly computing the transformed feature vectors. It allows SVMs to efficiently handle non-linearly separable data by effectively linearizing it in the higher-dimensional feature space.\n",
    "\n",
    "In traditional SVMs, the decision boundary is a linear hyperplane that separates the data points of different classes. However, many real-world datasets are not linearly separable. The kernel trick addresses this limitation by introducing a kernel function that calculates the inner product between two feature vectors in the transformed feature space.\n",
    "\n",
    "Mathematically, the kernel trick replaces the dot product of two feature vectors, x and x', with the evaluation of a kernel function, K(x, x'). The kernel function calculates the similarity between two data points in the original input space or a higher-dimensional feature space. By using different kernel functions, SVMs can implicitly operate in various feature spaces, including those of infinite dimensions.\n",
    "\n",
    "The kernel function satisfies Mercer's condition, which ensures that it corresponds to a valid dot product in some feature space. Popular kernel functions include:\n",
    "\n",
    "Linear Kernel: K(x, x') = x^T * x'\n",
    "This kernel performs the standard dot product between the input vectors, resulting in a linear SVM.\n",
    "\n",
    "Polynomial Kernel: K(x, x') = (gamma * x^T * x' + r)^d\n",
    "This kernel computes the polynomial similarity between the input vectors, where gamma, r, and d are parameters.\n",
    "\n",
    "Gaussian (RBF) Kernel: K(x, x') = exp(-gamma * ||x - x'||^2)\n",
    "The Gaussian kernel measures the similarity based on the Euclidean distance between the input vectors, where gamma is a parameter.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "738b283d-8084-4874-95ae-2a3e08f4e752",
   "metadata": {},
   "source": [
    "### 4)\n",
    "\n",
    "In Support Vector Machines (SVMs), support vectors are the data points from the training set that lie closest to the decision boundary or hyperplane. These support vectors play a crucial role in determining the decision boundary and are used to define the margin of the SVM.\n",
    "\n",
    "The main idea behind SVM is to find the hyperplane that maximizes the margin between the classes, while still correctly classifying the training data. The margin is the region between the decision boundary and the closest data points from each class. The support vectors are those data points that lie on the margin or are misclassified, and they directly influence the position and orientation of the decision boundary.\n",
    "\n",
    "The support vectors are critical because they define the margin of the SVM. The margin is the distance between the decision boundary and the support vectors on both sides. The SVM aims to maximize this margin while still correctly classifying the training data.\n",
    "\n",
    "During the training process, the SVM algorithm optimizes the position and orientation of the decision boundary based on the support vectors. Only the support vectors contribute to the decision boundary, while other data points that are not support vectors do not affect the boundary. This property makes SVMs memory efficient, as they only rely on a subset of the training data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb76059-a8fa-403c-94ce-e48b417d9003",
   "metadata": {},
   "outputs": [],
   "source": [
    "###"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
